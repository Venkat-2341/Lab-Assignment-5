{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from transformers import AutoFeatureExtractor, ResNetForImageClassification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Setup: Reproducibility & Device\n",
    "# ----------------------------\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Data Preparation\n",
    "# ----------------------------\n",
    "# Paths to datasets (adjust these paths to your dataset locations)\n",
    "original_train_dir = \"original_dataset/train\"\n",
    "original_test_dir = \"original_dataset/test\"\n",
    "augmented_train_dir = \"dataset/train\"\n",
    "# We'll use the original test set for evaluation.\n",
    "test_dir = original_test_dir\n",
    "\n",
    "# Load a feature extractor from Hugging Face to obtain normalization parameters\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")\n",
    "\n",
    "# Define transforms for training and testing (resize, crop, normalize)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "])\n",
    "\n",
    "# Create datasets using ImageFolder (requires folder structure: <root>/<class>/images)\n",
    "original_train_dataset = datasets.ImageFolder(root=original_train_dir, transform=train_transform)\n",
    "augmented_train_dataset  = datasets.ImageFolder(root=augmented_train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "batch_size = 32\n",
    "original_train_loader = DataLoader(original_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "augmented_train_loader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Function to count images per class in a dataset\n",
    "def count_dataset_images(dataset):\n",
    "    counts = {}\n",
    "    for _, label in dataset.samples:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "    return counts\n",
    "\n",
    "print(\"Original Train counts per class:\", count_dataset_images(original_train_dataset))\n",
    "print(\"Augmented Train counts per class:\", count_dataset_images(augmented_train_dataset))\n",
    "print(\"Test counts per class:\", count_dataset_images(test_dataset))\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Model Initialization\n",
    "# ----------------------------\n",
    "# Load the pretrained ResNet-50 model for image classification.\n",
    "# The pretrained model is originally built for 1000 classes (ImageNet).\n",
    "# To use it for 2 classes, we set num_labels=2 and add ignore_mismatched_sizes=True\n",
    "# so that the classification head is reinitialized.\n",
    "model = ResNetForImageClassification.from_pretrained(\n",
    "    \"microsoft/resnet-50\",\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(\"\\nModel Architecture:\\n\", model)\n",
    "\n",
    "# Save the initial state so both experiments start from the same weights\n",
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Training and Evaluation Functions\n",
    "# ----------------------------\n",
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits  # The model returns a ModelOutput; we use .logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).logits\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Experiments: Training on Original vs. Augmented Data\n",
    "# ----------------------------\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "results = {}\n",
    "\n",
    "# --- Experiment 1: Train on Original (Non-Augmented) Dataset ---\n",
    "print(\"\\n--- Training on Original (Non-Augmented) Dataset ---\")\n",
    "# Reset model to initial weights\n",
    "model.load_state_dict(initial_state_dict)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model = train_model(model, original_train_loader, criterion, optimizer, device, num_epochs=num_epochs)\n",
    "acc_orig, prec_orig, rec_orig, f1_orig = evaluate_model(model, test_loader, device)\n",
    "results['Original'] = {'Accuracy': acc_orig, 'Precision': prec_orig, 'Recall': rec_orig, 'F1': f1_orig}\n",
    "\n",
    "# --- Experiment 2: Train on Augmented Dataset ---\n",
    "print(\"\\n--- Training on Augmented Dataset ---\")\n",
    "# Reset model to initial weights\n",
    "model.load_state_dict(initial_state_dict)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model = train_model(model, augmented_train_loader, criterion, optimizer, device, num_epochs=num_epochs)\n",
    "acc_aug, prec_aug, rec_aug, f1_aug = evaluate_model(model, test_loader, device)\n",
    "results['Augmented'] = {'Accuracy': acc_aug, 'Precision': prec_aug, 'Recall': rec_aug, 'F1': f1_aug}\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Display and Plot Evaluation Metrics\n",
    "# ----------------------------\n",
    "print(\"\\nEvaluation Metrics on Test Set:\")\n",
    "for exp_name, metrics in results.items():\n",
    "    print(f\"\\n{exp_name} Training:\")\n",
    "    print(f\"  Accuracy : {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"  Recall   : {metrics['Recall']:.4f}\")\n",
    "    print(f\"  F1 Score : {metrics['F1']:.4f}\")\n",
    "\n",
    "# Plot a bar graph comparing the metrics from both experiments\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "original_metrics = [results['Original'][m] for m in metrics_names]\n",
    "augmented_metrics = [results['Augmented'][m] for m in metrics_names]\n",
    "\n",
    "x = np.arange(len(metrics_names))  # Label locations\n",
    "width = 0.35  # Bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, original_metrics, width, label='Original')\n",
    "bars2 = ax.bar(x + width/2, augmented_metrics, width, label='Augmented')\n",
    "\n",
    "ax.set_xlabel(\"Evaluation Metrics\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Comparison of Evaluation Metrics on the Test Set\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_names)\n",
    "ax.legend()\n",
    "\n",
    "# Function to attach value labels to each bar\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # Offset text vertically by 3 points\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(bars1)\n",
    "autolabel(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Explanation\n",
    "# ----------------------------\n",
    "print(\"\\nExplanation:\")\n",
    "print(\"Both experiments start with the same initial model weights to ensure a fair comparison.\")\n",
    "print(\"Data augmentation increases the number of training images (from 56 per class to 168 per class), which can help improve generalization.\")\n",
    "print(\"If the augmented training yields better evaluation metrics on the test set, it indicates improved robustness to variations in the data.\")\n",
    "print(\"If the results are similar or worse, further tuning of the augmentation parameters, training hyperparameters, or more training epochs might be needed.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
